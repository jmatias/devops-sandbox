{{ if .Values.datahub.enabled }}
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: datahub-prerequisites
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "0"
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  destination:
    namespace: datahub
    server: {{ .Values.spec.destination.server }}
  project: default
  source:
    path: infra/apps/datahub-prerequisites
    repoURL: {{ .Values.spec.source.repoURL }}
    targetRevision: main
    kustomize:
      namespace: datahub

  source:
    chart: datahub-prerequisites
    repoURL: https://helm.datahubproject.io/
    targetRevision: 0.*
    helm:
      releaseName: prerequisites
      valuesObject:
        # Default configuration for pre-requisites to get you started
        # Copy this file and update to the configuration of choice
        elasticsearch:
          # set this to false, if you want to provide your own ES instance.
          enabled: true

          # If you're running in production, set this to 3 and comment out antiAffinity below
          # Or alternatively if you're running production, bring your own ElasticSearch
          replicas: 1
          minimumMasterNodes: 1
          # Set replicas to 1 and uncomment this to allow the instance to be scheduled on
          # a master node when deploying on a single node Minikube / Kind / etc cluster.
          antiAffinity: "soft"

          # If you are running a multi-replica cluster, comment this out
          clusterHealthCheckParams: "wait_for_status=yellow&timeout=1s"

          # Shrink default JVM heap.
          esJavaOpts: "-Xmx512m -Xms512m"

          # Allocate smaller chunks of memory per pod.
          resources:
            requests:
              cpu: "100m"
              memory: "1024M"
            limits:
              cpu: "1000m"
              memory: "1024M"

          # Request smaller persistent volumes.
          volumeClaimTemplate:
            accessModes: [ "ReadWriteOnce" ]
            storageClassName: "gp2"
            resources:
              requests:
                storage: 100M

        # Official neo4j chart, supports both community and enterprise editions
        # see https://neo4j.com/docs/operations-manual/current/kubernetes/ for more information
        # source: https://github.com/neo4j/helm-charts

        mysql:
          enabled: true
          global:
            storageClass: "gp2"
            defaultStorageClass: "gp2"
          image:
            tag: 8.0.32-debian-11-r26
          auth:
            # For better security, add mysql-secrets k8s secret with mysql-root-password, mysql-replication-password and mysql-password
            existingSecret: mysql-secrets
          primary:
            extraFlags: "--character-set-server=utf8mb4 --collation-server=utf8mb4_bin"


        # Bitnami version of Kafka that deploys open source Kafka https://artifacthub.io/packages/helm/bitnami/kafka
        kafka:
          enabled: true
          global:
            storageClass: "gp2"
            defaultStorageClass: "gp2"
          listeners:
            client:
              protocol: PLAINTEXT
            interbroker:
              protocol: PLAINTEXT
          controller:
            replicaCount: 0
            logPersistence:
              storageClass: gp2
          broker:
            persistence:
              storageClass: gp2
            replicaCount: 1
            # The new minId for broker is 100. If we don't override this, the broker will have id 100
            # and cannot load the partitions. So we set minId to 0 to be backwards compatible
            minId: 0
            # These server properties are no longer exposed as parameters in the bitnami kafka chart since 24.0.0
            # They need to be passed in through extraConfig. See below for reference
            # https://github.com/bitnami/charts/tree/main/bitnami/kafka#to-2400
            extraConfig: |
              message.max.bytes=5242880
              default.replication.factor=1
              offsets.topic.replication.factor=1
              transaction.state.log.replication.factor=1
          kraft:
            enabled: false
          zookeeper:
            enabled: true
            persistence:
              storageClass: "gp2"

  syncPolicy:
    syncOptions:
      - CreateNamespace=true
      - PruneLast=false
      - ServerSideApply=false

    {{ if .Values.datahub.autoSync }}
    automated:
      prune: true
      selfHeal: true
    {{ end }}

    retry:
      limit: -1
      backoff:
        duration: 30s
        factor: 2
        maxDuration: 3m0s


{{- end }}
