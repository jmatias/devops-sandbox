---
apiVersion: external-secrets.io/v1
kind: ExternalSecret
metadata:
  name: mysql-secrets
  namespace: datahub
spec:
  refreshInterval: 24h
  secretStoreRef:
    name: cluster-secretstore
    kind: ClusterSecretStore
  target:
    name: mysql-secrets
    creationPolicy: Owner
  data:
    - secretKey: mysql-root-password
      remoteRef:
        key: javier/databhub/mysql
        property: mysql-root-password

    - secretKey: mysql-password
      remoteRef:
        key: javier/databhub/mysql
        property: mysql-password


---
apiVersion: external-secrets.io/v1
kind: ExternalSecret
metadata:
  name: neo4j-secrets
  namespace: datahub
spec:
  refreshInterval: 24h
  secretStoreRef:
    name: cluster-secretstore
    kind: ClusterSecretStore
  target:
    name: neo4j-secrets
    creationPolicy: Owner
  data:
    - secretKey: neo4j-password
      remoteRef:
        key: javier/datahub/neo4j
        property: neo4j-password
    - secretKey: NEO4J_AUTH
      remoteRef:
        key: javier/datahub/neo4j
        property: NEO4J_AUTH

---

{{ if .Values.datahub.enabled }}
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: datahub-prerequisites
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "0"
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  destination:
    namespace: datahub
    server: {{ .Values.spec.destination.server }}
  project: default
{{/*  source:*/}}
{{/*    path: infra/apps/datahub-prerequisites*/}}
{{/*    repoURL: {{ .Values.spec.source.repoURL }}*/}}
{{/*    targetRevision: main*/}}
{{/*    kustomize:*/}}
{{/*      namespace: datahub*/}}

  source:
    chart: datahub-prerequisites
    repoURL: https://helm.datahubproject.io/
    targetRevision: 0.*
    helm:
      releaseName: prerequisites
      valuesObject:
        # Default configuration for pre-requisites to get you started
        # Copy this file and update to the configuration of choice
        elasticsearch:
          # set this to false, if you want to provide your own ES instance.
          enabled: true

          # If you're running in production, set this to 3 and comment out antiAffinity below
          # Or alternatively if you're running production, bring your own ElasticSearch
          replicas: 1
          minimumMasterNodes: 1
          # Set replicas to 1 and uncomment this to allow the instance to be scheduled on
          # a master node when deploying on a single node Minikube / Kind / etc cluster.
          antiAffinity: "soft"

          # If you are running a multi-replica cluster, comment this out
          clusterHealthCheckParams: "wait_for_status=yellow&timeout=1s"

          # Shrink default JVM heap.
          esJavaOpts: "-Xmx512m -Xms512m"

          # Allocate smaller chunks of memory per pod.
          resources:
            requests:
              cpu: "100m"
              memory: "1024M"
            limits:
              cpu: "1000m"
              memory: "1024M"

          # Request smaller persistent volumes.
          volumeClaimTemplate:
            accessModes: [ "ReadWriteOnce" ]
            storageClassName: "gp2"
            resources:
              requests:
                storage: 100M

        # Official neo4j chart, supports both community and enterprise editions
        # see https://neo4j.com/docs/operations-manual/current/kubernetes/ for more information
        # source: https://github.com/neo4j/helm-charts
        neo4j:
          enabled: false
          nameOverride: neo4j
          neo4j:
            name: neo4j
            edition: "community"
            acceptLicenseAgreement: "yes"
            defaultDatabase: "graph.db"
            password: "datahub"
            # For better security, add password to neo4j-secrets k8s secret with: neo4j-username, neo4j-password, and NEO4J_AUTH then uncomment below
            # NEO4J_AUTH: should be composed like so: {Username}/{Password}
            # passwordFromSecret: neo4j-secrets

          # Set security context for pod
          securityContext:
            runAsNonRoot: true
            runAsUser: 7474
            runAsGroup: 7474
            fsGroup: 7474
            fsGroupChangePolicy: "Always"

          # Disallow privilegeEscalation on container level
          containerSecurityContext:
            allowPrivilegeEscalation: false

          # Create a volume for neo4j, SSD storage is recommended
          volumes:
            data:
              mode: "defaultStorageClass"

              defaultStorageClass:
                storageClassName: "gp2"
              # mode: "dynamic"
              # dynamic:
              #   storageClassName: managed-csi-premium

          env:
            NEO4J_PLUGINS: '["apoc"]'

        mysql:
          enabled: true
          global:
            storageClass: "gp2"
            defaultStorageClass: "gp2"
          image:
            tag: 8.0.32-debian-11-r26
          auth:
            # For better security, add mysql-secrets k8s secret with mysql-root-password, mysql-replication-password and mysql-password
            existingSecret: mysql-secrets
          primary:
            extraFlags: "--character-set-server=utf8mb4 --collation-server=utf8mb4_bin"


        # Bitnami version of Kafka that deploys open source Kafka https://artifacthub.io/packages/helm/bitnami/kafka
        kafka:
          enabled: true
          global:
            storageClass: "gp2"
            defaultStorageClass: "gp2"
          listeners:
            client:
              protocol: PLAINTEXT
            interbroker:
              protocol: PLAINTEXT
          controller:
            replicaCount: 0
            logPersistence:
              storageClass: gp2
          broker:
            persistence:
              storageClass: gp2
            replicaCount: 1
            # The new minId for broker is 100. If we don't override this, the broker will have id 100
            # and cannot load the partitions. So we set minId to 0 to be backwards compatible
            minId: 0
            # These server properties are no longer exposed as parameters in the bitnami kafka chart since 24.0.0
            # They need to be passed in through extraConfig. See below for reference
            # https://github.com/bitnami/charts/tree/main/bitnami/kafka#to-2400
            extraConfig: |
              message.max.bytes=5242880
              default.replication.factor=1
              offsets.topic.replication.factor=1
              transaction.state.log.replication.factor=1
          kraft:
            enabled: false
          zookeeper:
            enabled: true
            persistence:
              storageClass: "gp2"

  syncPolicy:
    syncOptions:
      - CreateNamespace=true
      - PruneLast=false
      - ServerSideApply=false

    {{ if .Values.datahub.autoSync }}
    automated:
      prune: true
      selfHeal: true
    {{ end }}

    retry:
      limit: -1
      backoff:
        duration: 30s
        factor: 2
        maxDuration: 3m0s


{{- end }}
